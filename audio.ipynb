{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo in uso: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Dispositivo in uso: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifica il percorso del dataset e delle classi, se necessario per il preprocessing.\n",
    "# Se devi fare inferenza su nuovi file, imposta dataset_directory con il percorso dei tuoi nuovi file.\n",
    "# Se non fai più training, puoi utilizzare lo stesso scaler e le stesse classi usate per il training originale.\n",
    "\n",
    "label_classes = [\"destra\", \"sinistra\", \"giù\", \"su\", \"silenzio\"]\n",
    "target_duration = 1.1  # Durata in secondi\n",
    "target_sample_rate = 16000\n",
    "\n",
    "# Percorso al modello pretrained\n",
    "model_path = \"simple_model.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello caricato correttamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AndreaFumagalli\\AppData\\Local\\Temp\\ipykernel_25832\\1705195398.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Definiamo la stessa architettura del modello utilizzata in fase di training,\n",
    "# in modo da poter caricare correttamente i pesi pre-addestrati.\n",
    "\n",
    "class SimpleAudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleAudioClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Stessa dimensione calcolata in precedenza:\n",
    "        # Input: 17600 campioni, dopo 3 pool: 17600/2/2/2 = 2200\n",
    "        # Canali: 64, quindi 64*2200 = 140800 input al primo FC.\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 2200, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Carichiamo il modello\n",
    "num_classes = len(label_classes)\n",
    "model = SimpleAudioClassifier(num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Modello caricato correttamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se disponi di uno scaler salvato in precedenza, caricalo qui.\n",
    "# Se non hai uno scaler pre-salvato, puoi crearne uno sulla base dei dati di training originali.\n",
    "# In caso non lo avessi, è possibile utilizzare i dati di training salvati o un fallback (niente scaling).\n",
    "\n",
    "# Esempio (commentato): caricamento da file se era stato salvato con joblib\n",
    "# from joblib import load\n",
    "# scaler = load(\"scaler.joblib\")\n",
    "\n",
    "# Se non hai uno scaler, lascia scaler = None e non fare scaling.\n",
    "scaler = None\n",
    "\n",
    "def preprocess_audio(file_path, target_sr=16000, desired_duration=1.1, scaler=None):\n",
    "    # Funzione per preprocessare un singolo file audio (simile a quella usata in training)\n",
    "    try:\n",
    "        audio_data, original_sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Resample se necessario\n",
    "        if original_sr != target_sr:\n",
    "            audio_data = librosa.resample(audio_data, orig_sr=original_sr, target_sr=target_sr)\n",
    "        \n",
    "        desired_length = int(desired_duration * target_sr)\n",
    "        \n",
    "        # Padding/Troncamento\n",
    "        if len(audio_data) > desired_length:\n",
    "            audio_data = audio_data[:desired_length]\n",
    "        elif len(audio_data) < desired_length:\n",
    "            audio_data = np.pad(audio_data, (0, desired_length - len(audio_data)), mode=\"constant\")\n",
    "        \n",
    "        # Scaling se disponibile\n",
    "        if scaler is not None:\n",
    "            audio_data = scaler.transform([audio_data])[0]\n",
    "        \n",
    "        return audio_data\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel preprocessing del file {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: test_dataset/suu.mp3 - Predizione: su (Confidenza: 1.00)\n"
     ]
    }
   ],
   "source": [
    "# In questa cella eseguiamo l'inferenza su uno o più file audio.\n",
    "# Esempio: passiamo un file audio preprocessato al modello per ottenere la predizione.\n",
    "\n",
    "def predict_single_file(model, file_path, label_classes, scaler=None, threshold=0.6):\n",
    "    audio_data = preprocess_audio(file_path, target_sr=target_sample_rate, desired_duration=target_duration, scaler=scaler)\n",
    "    if audio_data is None:\n",
    "        return None\n",
    "    \n",
    "    # Convertiamo in tensore e modello\n",
    "    audio_tensor = torch.tensor(audio_data, dtype=torch.float32).unsqueeze(0).unsqueeze(1).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(audio_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        conf, pred_idx = torch.max(probs, 1)\n",
    "    \n",
    "    conf = conf.item()\n",
    "    predicted_class = label_classes[pred_idx.item()]\n",
    "    \n",
    "    if conf >= threshold:\n",
    "        print(f\"File: {file_path} - Predizione: {predicted_class} (Confidenza: {conf:.2f})\")\n",
    "    else:\n",
    "        print(f\"File: {file_path} - Nessuna predizione chiara. Confidenza massima: {conf:.2f}\")\n",
    "\n",
    "# Esempio d'uso (sostituisci con un tuo file audio):\n",
    "predict_single_file(model, \"test_dataset/suu.mp3\", label_classes, scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attendi 3 secondi prima della registrazione...\n",
      "Inizio registrazione...\n",
      "Registrazione completata.\n",
      "Audio salvato come rec.wav\n",
      "Nessun comando chiaro rilevato.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n",
    "def record_and_predict(model, label_classes, file_name=\"temp_recording.wav\", sr=16000, record_duration=1.5, wait_seconds=3, threshold=0.6, scaler=None):\n",
    "    \"\"\"\n",
    "    Questa funzione attende wait_seconds secondi, registra l'audio per record_duration secondi,\n",
    "    lo salva in file_name (.wav), quindi effettua la predizione con il modello fornito.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Timer prima della registrazione\n",
    "    print(f\"Attendi {wait_seconds} secondi prima della registrazione...\")\n",
    "    time.sleep(wait_seconds)\n",
    "    print(\"Inizio registrazione...\")\n",
    "\n",
    "    # Registrazione dell'audio\n",
    "    recorded_audio = sd.rec(int(record_duration * sr), samplerate=sr, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    print(\"Registrazione completata.\")\n",
    "\n",
    "    # Salvare il file in formato wav\n",
    "    sf.write(file_name, recorded_audio, sr)\n",
    "    print(f\"Audio salvato come {file_name}\")\n",
    "\n",
    "    # Preprocessare l'audio (resample, padding, scaling se necessario)\n",
    "    # Dal momento che abbiamo già la frequenza di campionamento impostata, non serve resample.\n",
    "    # Se necessario, scaling:\n",
    "    audio_data = np.squeeze(recorded_audio)\n",
    "    \n",
    "    # Padding/Troncamento a target_duration se differente da record_duration:\n",
    "    # Supponiamo di voler usare la stessa durata target del training, ad esempio 1.1 secondi.\n",
    "    target_duration = 1.1\n",
    "    desired_length = int(target_duration * sr)\n",
    "    if len(audio_data) > desired_length:\n",
    "        audio_data = audio_data[:desired_length]\n",
    "    elif len(audio_data) < desired_length:\n",
    "        audio_data = np.pad(audio_data, (0, desired_length - len(audio_data)), mode='constant')\n",
    "\n",
    "    # Applichiamo lo scaling se abbiamo uno scaler\n",
    "    if scaler is not None:\n",
    "        audio_data = scaler.transform([audio_data])[0]\n",
    "\n",
    "    # Creazione del tensore per il modello\n",
    "    audio_tensor = torch.tensor(audio_data, dtype=torch.float32).unsqueeze(0).unsqueeze(1).to(device)\n",
    "\n",
    "    # Inferenza\n",
    "    with torch.no_grad():\n",
    "        output = model(audio_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "\n",
    "    conf_value = confidence.item()\n",
    "    predicted_label = label_classes[predicted_idx.item()]\n",
    "\n",
    "    if conf_value >= threshold and predicted_label != \"silenzio\":\n",
    "        print(f\"Comando riconosciuto: {predicted_label} (Conf: {conf_value:.2f})\")\n",
    "    else:\n",
    "        print(\"Nessun comando chiaro rilevato.\")\n",
    "\n",
    "# Esempio di utilizzo:\n",
    "record_and_predict(model, label_classes, file_name=\"my_recording.wav\", sr=16000, record_duration=1.5, wait_seconds=3, threshold=0.6, scaler=scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### PRETRAINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AndreaFumagalli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Users\\AndreaFumagalli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AndreaFumagalli\\.cache\\huggingface\\hub\\models--jonatasgrosman--wav2vec2-large-xlsr-53-italian. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trascrizione ottenuta: 'destra'\n",
      "Comando riconosciuto: destra\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "model_name = \"jonatasgrosman/wav2vec2-large-xlsr-53-italian\"\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(\"cpu\").eval()\n",
    "\n",
    "commands = [\"destra\", \"sinistra\", \"su\", \"giù\"]\n",
    "\n",
    "def transcribe_audio(file_path, processor, model, target_sr=16000):\n",
    "    audio_data, sr = librosa.load(file_path, sr=None)\n",
    "    if sr != target_sr:\n",
    "        audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=target_sr)\n",
    "    input_values = processor(audio_data, sampling_rate=target_sr, return_tensors=\"pt\").input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    return transcription.lower()\n",
    "\n",
    "file_to_predict = \"rec.wav\"\n",
    "transcription = transcribe_audio(file_to_predict, processor, model)\n",
    "print(f\"Trascrizione ottenuta: '{transcription}'\")\n",
    "\n",
    "detected_command = None\n",
    "for cmd in commands:\n",
    "    if cmd in transcription:\n",
    "        detected_command = cmd\n",
    "        break\n",
    "\n",
    "if detected_command is not None:\n",
    "    print(f\"Comando riconosciuto: {detected_command}\")\n",
    "else:\n",
    "    print(\"Nessun comando tra destra, sinistra, su, giù è stato rilevato nella trascrizione.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
